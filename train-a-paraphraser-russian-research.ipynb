{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'","metadata":{"execution":{"iopub.status.busy":"2022-07-18T15:59:52.186044Z","iopub.execute_input":"2022-07-18T15:59:52.186819Z","iopub.status.idle":"2022-07-18T15:59:52.214448Z","shell.execute_reply.started":"2022-07-18T15:59:52.186720Z","shell.execute_reply":"2022-07-18T15:59:52.213528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the data is not here, uncomment the lines below and download it","metadata":{}},{"cell_type":"code","source":"# !wget https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip\n# !unzip filtered_paranmt.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train a paraphraser on the mined data","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('filtered.tsv', sep='\\t', encoding='utf-8')\nprint(df.shape)\ndf.sample(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df.ref_tox > df.trn_tox).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = []\nyy = []\nfor i, row in df.iterrows():\n    if row.ref_tox > row.trn_tox:\n        xx.append(row.reference)\n        yy.append(row.translation)\n    else:\n        yy.append(row.reference)\n        xx.append(row.translation)\n        \nxydf = pd.DataFrame({'source': xx, 'target': yy})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare datasets","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer, T5TokenizerFast,\n    get_linear_schedule_with_warmup\n)\nimport torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"ceshine/t5-paraphrase-paws-msrp-opinosis\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5TokenizerFast.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = train_test_split(xydf, test_size=300)\nprint(df_train.shape[0], df_test.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nx1 = tokenizer(df_train.source.tolist(), truncation=True)\ny1 = tokenizer(df_train.target.tolist(), truncation=True)\nx2 = tokenizer(df_test.source.tolist(), truncation=True)\ny2 = tokenizer(df_test.target.tolist(), truncation=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PairsDataset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __getitem__(self, idx):\n        assert idx < len(self.x['input_ids'])\n        item = {key: val[idx] for key, val in self.x.items()}\n        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n        item['labels'] = self.y['input_ids'][idx]\n        return item\n    \n    @property\n    def n(self):\n        return len(self.x['input_ids'])\n\n    def __len__(self):\n        return self.n # * 2\n    \ntrain_dataset = PairsDataset(x1, y1)\ntest_dataset = PairsDataset(x2, y2)\nlen(train_dataset), len(test_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=1)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, drop_last=True, shuffle=True, num_workers=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tune t5","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)\nimport torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_name = 'SkolkovoInstitute/t5-paraphrase-paws-msrp-opinosis-paranmt'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(checkpoint_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0')\nmodel.to(device);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom transformers.file_utils import cached_property\nfrom typing import Tuple\n\nclass TrAr(TrainingArguments):\n    @cached_property\n    def _setup_devices(self):\n        return device","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Union\n\nclass DataCollatorWithPadding:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        batch = self.tokenizer.pad(\n            features,\n            padding=True,\n        )\n        ybatch = self.tokenizer.pad(\n            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n            padding=True,\n        ) \n        batch['labels'] = ybatch['input_ids']\n        batch['decoder_attention_mask'] = ybatch['attention_mask']\n        \n        return {k: torch.tensor(v) for k, v in batch.items()}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_name = 'models/t5-cechine-nmt-mined-detox'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"todo: maybe, batch > 4 would do as well","metadata":{}},{"cell_type":"code","source":"training_args = TrAr(\n    output_dir=save_name,   # output directory\n    overwrite_output_dir=True,\n    num_train_epochs=3,             # total # of training epochs\n    per_device_train_batch_size=4,  # batch size per device during training\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=8,    # batch size for evaluation\n    warmup_steps=300,               # number of warmup steps for learning rate scheduler\n    weight_decay=0,                  # strength of weight decay\n    learning_rate=3e-5,\n    logging_dir='./logs',           # directory for storing logs\n    logging_steps=100,\n    eval_steps=100,\n    evaluation_strategy='steps',\n    save_total_limit=1,\n    save_steps=5000,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset,           # evaluation dataset\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer('The internal policy of the fucking Trump is stupid.', return_tensors='pt')\ninputs = {k: v.to(device) for k, v in inputs.items()}\nfor t in model.generate(**inputs, num_return_sequences=10, do_sample=False, num_beams=10):\n    print(tokenizer.decode(t, skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(save_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}